import mysql.connector
import schedule
import time
import threading
from datetime import datetime, timedelta
from slack_sdk import WebClient
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
import pandas as pd
import os
import numpy as np
import joblib
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib import rc
from tensorflow.keras.models import load_model
from tensorflow.keras.losses import MeanSquaredError
import re


# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (ìš´ì˜ì²´ì œë³„ ìë™ ì ìš©)
plt.rc('font', family='Malgun Gothic')  # Windows (ë§‘ì€ ê³ ë”•)
plt.rc('axes', unicode_minus=False)  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# âœ… ëª¨ë¸ ë° ì •ê·œí™” ë„êµ¬ ë¡œë“œ
models = {
    "CU": load_model("test/lstm_CU_model.h5", custom_objects={"mse": MeanSquaredError()}),
    "GS25": load_model("test/lstm_GS25_model.h5", custom_objects={"mse": MeanSquaredError()}),
    "Seven": load_model("test/lstm_Seven_model.h5", custom_objects={"mse": MeanSquaredError()})
}
scalers_X = {
    "CU": joblib.load("test/scaler_CU_X.pkl"),
    "GS25": joblib.load("test/scaler_GS25_X.pkl"),
    "Seven": joblib.load("test/scaler_Seven_X.pkl")
}
scalers_y = {
    "CU": joblib.load("test/scaler_CU_y.pkl"),
    "GS25": joblib.load("test/scaler_GS25_y.pkl"),
    "Seven": joblib.load("test/scaler_Seven_y.pkl")
}

# âœ… ë§¤ì¶œ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_sales(convenience_store, date_to_predict):
    model = models[convenience_store]
    scaler_X = scalers_X[convenience_store]
    scaler_y = scalers_y[convenience_store]

    # âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    file_path = f"test/all_sale_{convenience_store}.xlsx"
    df = pd.read_excel(file_path)
    df['sale_date'] = pd.to_datetime(df['sale_date'])
    df = df.sort_values(by='sale_date')

    # âœ… ëª¨ë¸ì´ í•™ìŠµí•œ ì»¬ëŸ¼ í™•ì¸
    expected_features = scaler_X.feature_names_in_

    # âœ… 7ì¼ ì´ë™ í‰ê·  ê³„ì‚° (ëˆ„ë½ëœ ì»¬ëŸ¼ ìë™ ì¶”ê°€)
    rolling_columns = {
        '1+1_event_count': '1+1_7ì¼í‰ê· ',
        '2+1_event_count': '2+1_7ì¼í‰ê· ',
        'event_img': 'ì˜ˆëŠ¥_7ì¼í‰ê· '
    }
    for original_col, rolling_col in rolling_columns.items():
        if original_col in df.columns and rolling_col not in df.columns:
            df[rolling_col] = df[original_col].rolling(window=7, min_periods=1).mean()

    for col in ['sum_amount', 'man10', 'man20', 'man30', 'man40', 'man50', 'man60',
                'woman10', 'woman20', 'woman30', 'woman40', 'woman50', 'woman60']:
        df[f'{col}_7ì¼í‰ê· '] = df[col].rolling(window=7, min_periods=1).mean()

    df['store_count_7ì¼í‰ê· '] = df['store_count'].rolling(window=7, min_periods=1).mean()

    # âœ… ìµœê·¼ 7ì¼ ë°ì´í„° ì„ íƒ
    recent_data = df[df['sale_date'] < date_to_predict].iloc[-7:].copy()

    # âœ… ë°ì´í„° ê²€ì¦
    if recent_data.empty or len(recent_data) < 7:
        return "âŒ ì˜ˆì¸¡ì„ ìœ„í•œ ìµœê·¼ 7ì¼ì¹˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

    # âœ… ëª¨ë¸ì´ í•™ìŠµí•œ ì»¬ëŸ¼ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë³€í™˜
    X_input = recent_data[expected_features]

    # âœ… ì •ê·œí™” ë° ì˜ˆì¸¡ ìˆ˜í–‰
    X_input_scaled = scaler_X.transform(X_input)
    X_input_reshaped = np.array([X_input_scaled])
    y_pred_scaled = model.predict(X_input_reshaped)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)

    # âœ… ê²°ê³¼ í¬ë§·íŒ…
    columns = ['sum_amount', 'man10', 'man20', 'man30', 'man40', 'man50', 'man60',
               'woman10', 'woman20', 'woman30', 'woman40', 'woman50', 'woman60']
    predicted_values = {col: y_pred[0][i] for i, col in enumerate(columns)}

    # âœ… í•­ì•„ë¦¬ ì°¨íŠ¸ ìƒì„±
    graph_path = generate_gender_sales_graph(convenience_store, date_to_predict, predicted_values)

    # âœ… Slack ë©”ì‹œì§€ êµ¬ì„±
    message = format_prediction_message(convenience_store, date_to_predict, predicted_values)

    return message, graph_path

# âœ… í•­ì•„ë¦¬ ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜
def generate_gender_sales_graph(convenience_store, date_to_predict, predicted_values):
    age_groups = ["10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€", "60ëŒ€ ì´ìƒ"]
    male_values = [predicted_values[f'man{i}0'] for i in range(1, 7)]
    female_values = [predicted_values[f'woman{i}0'] for i in range(1, 7)]

    male_values = np.array(male_values) / 1e8  # ì–µ ì› ë‹¨ìœ„ ë³€í™˜
    female_values = np.array(female_values) / 1e8  # ì–µ ì› ë‹¨ìœ„ ë³€í™˜

    plt.figure(figsize=(8, 6))
    plt.barh(age_groups, male_values, color="#2a50ae", label="ë‚¨ì„±")  # ì–´ë‘ìš´ íŒŒë€ìƒ‰
    plt.barh(age_groups, -female_values, color="#c20000", label="ì—¬ì„±")  # ì–´ë‘ìš´ ë¹¨ê°„ìƒ‰

    plt.axvline(x=0, color="black", linewidth=1)  # ê°€ìš´ë° ê¸°ì¤€ì„  ì¶”ê°€
    plt.xlabel("ë§¤ì¶œ (ì–µ ì›)")
    plt.title(f"{convenience_store} {date_to_predict} ì„±ë³„ ë§¤ì¶œ ì˜ˆì¸¡")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)

    # âœ… xì¶•ì„ ì–µ ì› ë‹¨ìœ„ë¡œ ë³€í™˜
    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f"{x:.1f}ì–µ"))

    plt.savefig(f"test/{convenience_store}_{date_to_predict}.png", bbox_inches="tight")
    plt.close()

    return f"test/{convenience_store}_{date_to_predict}.png"

# âœ… Slack ë©”ì‹œì§€ í¬ë§·íŒ…
def format_prediction_message(convenience_store, date_to_predict, predicted_values):
    total_sales = predicted_values["sum_amount"]
    male_total = sum(predicted_values[f"man{i}0"] for i in range(1, 7))
    female_total = sum(predicted_values[f"woman{i}0"] for i in range(1, 7))

    message = f"*ğŸ“¢ {convenience_store} {date_to_predict} ë§¤ì¶œ ì˜ˆì¸¡ ğŸ“¢*\n"
    message += f"ì´ ë§¤ì¶œ: {total_sales:,.0f} ì›\n"
    message += f"  ğŸ‘¨ğŸ» ë‚¨ì ì´ ë§¤ì¶œì•¡: {male_total:,.0f} ì›\n"
    for i in range(1, 7):
        message += f"        {i}0ëŒ€ ë‚¨ì: {predicted_values[f'man{i}0']:,.0f} ì›\n"
    message += f"  ğŸ‘©ğŸ» ì—¬ì ì´ ë§¤ì¶œì•¡: {female_total:,.0f} ì›\n"
    for i in range(1, 7):
        message += f"        {i}0ëŒ€ ì—¬ì: {predicted_values[f'woman{i}0']:,.0f} ì›\n"

    return message

# ğŸŸ¦ Slack API Token ì„¤ì •
slack_token = "xoxb---"
slack_app_token = "xapp-1---"
client = WebClient(token=slack_token)

# ğŸŸ¦ MySQL ì—°ê²° ì„¤ì •
conn = mysql.connector.connect(
    host='3.35.236.56',
    port=3306,
    user='kdy',
    password='0710',
    database='crawling',
    charset='utf8mb4'
)
# ğŸŸ¦ Slack ì•± ì´ˆê¸°í™”
app = App(token=slack_token)


# ğŸŸ¦ ì±„ë„ ID ì„¤ì •
GS25_CHANNEL_ID = "C08BYUVRW4X"
CU_CHANNEL_ID = "C08CG17U15J"
SEVEN_CHANNEL_ID = "C08DJJT0Y8G"
YOUTUBE_CHANNEL_ID = "C08D3PN7SRL"
NAVER_NEWS_CHANNEL_ID = "C08DQRKQR51"
ALERT_CHANNEL_ID = "C08DRRPN61Z"
PREV_CHANNEL_ID = "C08EX9LFZT6"

# ğŸŸ¦ í¸ì˜ì  ì±„ë„ ë§¤í•‘
store_mapping = {
    GS25_CHANNEL_ID: "GS25",
    CU_CHANNEL_ID: "CU",
    SEVEN_CHANNEL_ID: "seven"
}

# ğŸŸ¦ ë§¤ì¶œ ì´ìƒ ê°ì§€ ê¸°ì¤€ (growth_deviation ê¸°ì¤€)
ANOMALY_THRESHOLDS = {
    "CU": {"high": 1.252, "low": -1.344},
    "GS25": {"high": 1.135, "low": -1.461},
    "seven": {"high": 1.394, "low": -1.374},
}

# ğŸŸ¦ íŠ¹ì • ë‚ ì§œì˜ ìœ íŠœë¸Œ ë° ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ì¡°íšŒ (ìµœëŒ€ 3ê°œë§Œ ë¨¼ì € ë°˜í™˜)
def get_ytb_links_by_date_and_store(date, store_type):
    query = f"""
        SELECT video_url FROM ytb_video
        WHERE store_type = '{store_type}' AND published_at LIKE '{date}%';
    """
    cursor = conn.cursor(dictionary=True)
    cursor.execute(query)
    data = cursor.fetchall()
    cursor.close()
    
    return [row["video_url"] for row in data] if data else []  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜


# ğŸŸ¦ íŠ¹ì • ë‚ ì§œì˜ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ì¡°íšŒ (ìµœëŒ€ 3ê°œë§Œ ë¨¼ì € ë°˜í™˜)
def get_news_links_by_date_and_store(date, store_type):
    query = f"""
        SELECT news_url FROM news_search
        WHERE store_type = '{store_type}' AND news_date LIKE '{date}%'
        ORDER BY news_start DESC;
    """
    cursor = conn.cursor(dictionary=True)
    cursor.execute(query)
    data = cursor.fetchall()
    cursor.close()
    
    return [row["news_url"] for row in data] if data else []


# ğŸŸ¦ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘ ë‚ ì§œ (2024ë…„ 10ì›” 1ì¼ë¶€í„° ì‹œì‘)
simulation_date = datetime(2024, 10, 1)

def format_news_links(news_links, date, store_type):
    """ì²˜ìŒ 3ê°œ ë‰´ìŠ¤ ë§í¬ë§Œ ì¶œë ¥í•˜ê³ , ë”ë³´ê¸° ë²„íŠ¼ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    if not news_links:
        return "ğŸ“° ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.", None

    top_news = "\n".join(news_links[:3])  # ì²˜ìŒ 3ê°œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    more_news = news_links[3:]  # ë‚˜ë¨¸ì§€ ë‰´ìŠ¤ ë§í¬ëŠ” ë”ë³´ê¸° ë²„íŠ¼ì„ í†µí•´ ì¶œë ¥

    message = f"ğŸ“° {date} {store_type} ê´€ë ¨ ë‰´ìŠ¤: ğŸ¬\n" + top_news
    
    # ë”ë³´ê¸° ë²„íŠ¼ ì¶”ê°€
    attachments = None
    if more_news:
        attachments = [
            {
                "blocks": [
                    {
                        "type": "actions",
                        "elements": [
                            {
                                "type": "button",
                                "text": {"type": "plain_text", "text": "ë‰´ìŠ¤ ë”ë³´ê¸°"},
                                "action_id": "show_more_news",
                                "value": f"{date}|{store_type}"
                            }
                        ]
                    }
                ]
            }
        ]
    return message, attachments

def format_ytb_links(ytb_links, date, store_type):
    """ì²˜ìŒ 3ê°œ ìœ íŠœë¸Œ ë§í¬ë§Œ ì¶œë ¥í•˜ê³ , ë”ë³´ê¸° ë²„íŠ¼ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    if not ytb_links:
        return "ğŸ¬ ê´€ë ¨ ìœ íŠœë¸Œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", None

    top_ytb = "\n".join(ytb_links[:3])  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
    more_ytb = ytb_links[3:]  # ë‚˜ë¨¸ì§€ ìœ íŠœë¸Œ ë§í¬ëŠ” ë”ë³´ê¸° ë²„íŠ¼ì„ í†µí•´ ì¶œë ¥

    message = f"ğŸ¬ {date} {store_type} ìœ íŠœë¸Œ ì˜ìƒ ëª©ë¡ ğŸ¬\n" + top_ytb
    
    # ë”ë³´ê¸° ë²„íŠ¼ ì¶”ê°€
    attachments = None
    if more_ytb:
        attachments = [
            {
                "blocks": [
                    {
                        "type": "actions",
                        "elements": [
                            {
                                "type": "button",
                                "text": {"type": "plain_text", "text": "ìœ íŠœë¸Œ ë”ë³´ê¸°"},
                                "action_id": "show_more_ytb",
                                "value": f"{date}|{store_type}"
                            }
                        ]
                    }
                ]
            }
        ]
    return message, attachments

def detect_sales_anomalies():
    global simulation_date
    today_str = simulation_date.strftime("%Y-%m-%d")

    print(f"ğŸ” [LOG] {today_str} ë§¤ì¶œ ì´ìƒ ê°ì§€ ì²´í¬ ì¤‘...")

    query = f"""
        SELECT sale_date, store_type, sum_amount, growth_deviation
        FROM all_sale
        WHERE sale_date = '{today_str}';
    """
    cursor = conn.cursor(dictionary=True)
    cursor.execute(query)
    data = cursor.fetchall()
    cursor.close()

    if not data:
        print(f"âš ï¸ [LOG] {today_str} ë°ì´í„° ì—†ìŒ")
        return

    for row in data:
        store = row["store_type"]
        deviation = row["growth_deviation"]

        print(f"ğŸ” [DEBUG] {today_str} {store}: growth_deviation={deviation:.2f}")

        if store in ANOMALY_THRESHOLDS:
            ytb_links = get_ytb_links_by_date_and_store(today_str, store)
            news_links = get_news_links_by_date_and_store(today_str, store)

            ytb_message, ytb_attachments = format_ytb_links(ytb_links, today_str, store)
            news_message, news_attachments = format_news_links(news_links, today_str, store)

            if deviation >= ANOMALY_THRESHOLDS[store]["high"]:
                msg = (
                    f"ğŸš¨ {today_str} {store} ë§¤ì¶œ ê¸‰ë“± ê°ì§€!\n"
                    f"ğŸ’° ë§¤ì¶œì•¡: {row['sum_amount']:,.0f}ì›\n"
                    f"ğŸ“Š í¸ì°¨ (growth_deviation): {deviation:.2f}%\n\n"
                    f"{ytb_message}\n\n"
                    f"{news_message}\n"
                )

                client.chat_postMessage(
                    channel=ALERT_CHANNEL_ID,
                    text=msg,
                    attachments=(ytb_attachments if ytb_attachments else []) +
                                (news_attachments if news_attachments else []),
                    unfurl_links=True,
                    unfurl_media=True
                )
                print(f"âœ… [LOG] {store} ë§¤ì¶œ ê¸‰ë“± ê°ì§€: {deviation:.2f}%")

            if deviation <= ANOMALY_THRESHOLDS[store]["low"]:
                msg = (
                    f"âš ï¸ {today_str} {store} ë§¤ì¶œ ê¸‰ê° ê°ì§€!\n"
                    f"ğŸ’° ë§¤ì¶œì•¡: {row['sum_amount']:,.0f}ì›\n"
                    f"ğŸ“‰ í¸ì°¨ (growth_deviation): {deviation:.2f}%\n\n"
                    f"{ytb_message}\n\n"
                    f"{news_message}\n"
                )
                print(f"ğŸ“¨ [DEBUG] ë©”ì‹œì§€ ë‚´ìš©: {msg}")
                print(f"ğŸ“ [DEBUG] ì²¨ë¶€ íŒŒì¼: {(ytb_attachments if ytb_attachments else []) + (news_attachments if news_attachments else [])}")

                client.chat_postMessage(
                    channel=ALERT_CHANNEL_ID,
                    text=msg,
                    attachments=(ytb_attachments if ytb_attachments else []) +
                                (news_attachments if news_attachments else []),
                    unfurl_links=True,
                    unfurl_media=True
                )
                print(f"âœ… [LOG] {store} ë§¤ì¶œ ê¸‰ê° ê°ì§€: {deviation:.2f}%")

    simulation_date += timedelta(days=1)  # ë‚ ì§œ ì¦ê°€



# ğŸŸ¦ ë°±ê·¸ë¼ìš´ë“œì—ì„œ 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
def schedule_test_check():
    schedule.every(1).minutes.do(detect_sales_anomalies)
    while True:
        schedule.run_pending()
        time.sleep(10)

# ğŸŸ¦ ë§¤ì¶œ ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜
def get_sales_data(date, store_type):
    query = f"""
        SELECT sum_amount, sum_amount_growth, avg_sum_amount_growth, growth_deviation
        FROM all_sale
        WHERE sale_date = '{date}' AND store_type = '{store_type}';
    """
    cursor = conn.cursor(dictionary=True)
    cursor.execute(query)
    data = cursor.fetchone()
    cursor.close()
    return data

# ğŸŸ¦ ë©˜ì…˜ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# âœ… Slack ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (ë©˜ì…˜ ì²˜ë¦¬)
@app.event("app_mention")
def handle_mention(event, say, client):
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logging.debug(f"ğŸ“¢ ì´ë²¤íŠ¸ ê°ì§€: {event}")  # ì´ë²¤íŠ¸ ë¡œê·¸ ì¶œë ¥

    text = event.get("text", "").strip()
    text = re.sub(r"<@U[A-Z0-9]+>", "", text).strip().upper()
    channel_id = event.get("channel")

    # ğŸ”¹ "ëŒ€ì‹œë³´ë“œ" ëª…ë ¹ì–´ ì²˜ë¦¬
    if text == "ëŒ€ì‹œë³´ë“œ" and channel_id == "C08E48KQWET":
        say(f"ğŸ“Š íƒœë¸”ë¡œ ëŒ€ì‹œë³´ë“œ ë§í¬ì…ë‹ˆë‹¤: https://public.tableau.com/app/profile/.70256853/viz/shared/497MCJW64")
        return

    if text == "í‹°í”¼" and channel_id == "C08E48KQWET":
        say(f"ğŸ“Š íƒœë¸”ë¡œ TP ë§í¬ì…ë‹ˆë‹¤: https://public.tableau.com/app/profile/.70256853/viz/_17404727845250/3store_TP?publish=yes")
        return

    # ğŸ”¹ ì…ë ¥ì—ì„œ ë‚ ì§œ ë° í¸ì˜ì  ì¢…ë¥˜ ì¶”ì¶œ
    date_match = re.search(r"\d{4}-\d{2}-\d{2}", text)
    store_match = re.search(r"(GS25|CU|SEVEN)", text)

    # ğŸ”¹ ë§Œì•½ ì‚¬ìš©ìê°€ ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šì•˜ë‹¤ë©´
    if not date_match and not store_match:
        if channel_id in store_mapping:
            say("âš ï¸ ì‚¬ìš©ë²•: `@ë¸Œë‹ˆ GS25 2024-10-02` í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif channel_id in [YOUTUBE_CHANNEL_ID, NAVER_NEWS_CHANNEL_ID]:
            say("âš ï¸ ì‚¬ìš©ë²•: `@ë¸Œë‹ˆ GS25 2024-10-02` í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # ğŸ”¹ ë‚ ì§œì™€ í¸ì˜ì  ì¢…ë¥˜ ì¶”ì¶œ
    date = date_match.group(0) if date_match else None
    store_type = store_match.group(0) if store_match else None

    # âœ… ë§¤ì¶œ ì˜ˆì¸¡ ì‹¤í–‰
    if channel_id == PREV_CHANNEL_ID:
        date_match = re.search(r"\d{4}-\d{2}-\d{2}", text)
        store_match = re.search(r"(GS25|CU|SEVEN)", text)

        if date_match and store_match:
            date = date_match.group(0)
            store_type = store_match.group(0)

            # âœ… store_mappingì„ ì‚¬ìš©í•˜ì—¬ store_type ë³€í™˜
            store_type = "Seven" if store_type == "SEVEN" else store_type

            say(f"ğŸ“… {date} {store_type} ë§¤ì¶œ ì˜ˆì¸¡ ì¤‘...â³")

            try:
                response_message, graph_path = predict_sales(store_type, date)
                say(response_message)

                # âœ… ì˜ˆì¸¡ ê·¸ë˜í”„ ì—…ë¡œë“œ
                client.files_upload_v2(
                    channels=channel_id,
                    file=graph_path,
                    title=f"{store_type} {date} ë§¤ì¶œ ì˜ˆì¸¡ ê·¸ë˜í”„"
                )
            except Exception as e:
                say(f"âš ï¸ ë§¤ì¶œ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            say("âš ï¸ ì‚¬ìš©ë²•: `@ë¸Œë‹ˆ GS25 2024-10-02` í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # ğŸ”¹ ì±„ë„ì´ ì˜¬ë°”ë¥¸ ê²½ìš° ê¸°ì¡´ ë§¤ì¶œ ë°ì´í„° ì¡°íšŒ
    if channel_id in store_mapping:
        store_type = store_mapping[channel_id]
        if date:
            data = get_sales_data(date, store_type)
            if data:
                response = (
                    f"ğŸ“… {date} {store_type} ë§¤ì¶œ ë¶„ì„ ğŸ“Š\n"
                    f"ğŸ’° ë§¤ì¶œì•¡: {data['sum_amount']:,.0f}ì›\n"
                    f"ğŸ“ˆ ì „ë‚  ëŒ€ë¹„ ìƒìŠ¹ë¥ : {data['sum_amount_growth']:.2f}%\n"
                    f"ğŸ“Š 3ì‚¬ í‰ê·  ìƒìŠ¹ë¥ : {data['avg_sum_amount_growth']:.2f}%\n"
                    f"âš ï¸ í‰ê·  ëŒ€ë¹„ ì°¨ì´: {data['growth_deviation']:.2f}%"
                )
                say(response)
            else:
                say(f"âš ï¸ {date}ì— ëŒ€í•œ {store_type} ë§¤ì¶œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            say("âš ï¸ ì‚¬ìš©ë²•: `@ë¸Œë‹ˆ 2024-10-02` í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # ğŸ”¹ ìœ íŠœë¸Œ ê²€ìƒ‰ ì±„ë„ì—ì„œ ì‹¤í–‰
    elif channel_id == YOUTUBE_CHANNEL_ID:
        ytb_links = get_ytb_links_by_date_and_store(date, store_type)
        message, attachments = format_ytb_links(ytb_links, date, store_type)
        if ytb_links:
            say(message,attachments=attachments)
        else:
            say(f"âš ï¸ {date}ì— ëŒ€í•œ {store_type} ìœ íŠœë¸Œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸ”¹ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì±„ë„ì—ì„œ ì‹¤í–‰
    elif channel_id == NAVER_NEWS_CHANNEL_ID:
        news_links = get_news_links_by_date_and_store(date, store_type)
        message, attachments = format_news_links(news_links, date, store_type)

        if news_links:
            say(message,attachments=attachments)
        else:
            say(f"âš ï¸ {date}ì— ëŒ€í•œ {store_type} ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ğŸŸ¦ ìœ íŠœë¸Œ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
@app.action("show_more_ytb")
def handle_show_more_ytb(ack, body, respond):
    ack()
    
    value = body['actions'][0]['value']
    date, store_type = value.split("|")
    
    ytb_urls = get_ytb_links_by_date_and_store(date, store_type)

    if len(ytb_urls) > 3:
        more_ytb = "\n".join(ytb_urls[3:])  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜
        message = f"ğŸ¬ {date} {store_type} ì¶”ê°€ ìœ íŠœë¸Œ ì˜ìƒ ğŸ¬\n{more_ytb}"
        
        # Slackì—ì„œ ë§í¬ ë¯¸ë¦¬ë³´ê¸°ë¥¼ í™œì„±í™”í•˜ë„ë¡ ì„¤ì •
        respond(text=message, replace_original=False, unfurl_links=True, unfurl_media=True)
    else:
        respond(text="âš ï¸ ë” ì´ìƒ ìœ íŠœë¸Œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.", replace_original=False)


    
# ğŸŸ¦ ë‰´ìŠ¤ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
@app.action("show_more_news")
def handle_show_more_news(ack, body, respond):
    ack()
    
    value = body['actions'][0]['value']
    date, store_type = value.split("|")
    
    news_urls = get_news_links_by_date_and_store(date, store_type)
    
    if len(news_urls) > 3:
        more_news = news_urls[3:]
        message = f"ğŸ“° {date} {store_type} ì¶”ê°€ ë‰´ìŠ¤ ê¸°ì‚¬ ğŸ“°\n" + "\n".join(more_news)
        
        # Slackì—ì„œ ë§í¬ ë¯¸ë¦¬ë³´ê¸°ë¥¼ í™œì„±í™”í•˜ë„ë¡ ì„¤ì •
        respond(text=message, replace_original=False, unfurl_links=True, unfurl_media=True)
    else:
        respond(text="âš ï¸ ë” ì´ìƒ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.", replace_original=False)



# ğŸŸ¦ ì‹¤í–‰
if __name__ == "__main__":
    # Slack ì•± ì‹¤í–‰ê³¼ ë™ì‹œì— ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •
    threading.Thread(target=schedule_test_check, daemon=True).start()
    # Slack ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì‹¤í–‰
    handler = SocketModeHandler(app, slack_app_token)
    handler.start()
    # ë©”ì¸ ìŠ¤ë ˆë“œë¥¼ ìœ ì§€í•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì´ ì¢…ë£Œë˜ì§€ ì•Šë„ë¡ í•¨
    while True:
        time.sleep(1)
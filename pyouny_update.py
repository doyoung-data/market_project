import requests
from bs4 import BeautifulSoup
import pymysql
from datetime import datetime
import re
import schedule
import time

# ë¸Œëœë“œë³„ ê¸°ë³¸ URL ì„¤ì •
base_urls = {
    "GS25": "https://pyony.com/brands/gs25/?page=",
    "CU": "https://pyony.com/brands/cu/?page=",
    "Seven": "https://pyony.com/brands/seven/?page="
}

# HTTP ìš”ì²­ í—¤ë”
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# DB ì—°ê²° ì„¤ì •
DB_CONFIG = {
    "host": "3.35.236.56",
    "user": "jsh",
    "password": "0929",
    "database": "crawling",
    "charset": "utf8mb4"
}

# ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
def format_update_date(date_str):
    if "." in date_str:
        current_year = datetime.now().year
        return f"{current_year}-{date_str.replace('.', '-')}"
    return date_str

# ê°€ê²© ë³€í™˜ í•¨ìˆ˜
def format_price(price_str):
    price_match = re.search(r"\d+", price_str.replace(",", ""))
    return int(price_match.group()) if price_match else 0

# ë¸Œëœë“œë³„ ìµœì‹  update_date ê°€ì ¸ì˜¤ê¸°
def get_latest_update_date(brand):
    conn = pymysql.connect(**DB_CONFIG)
    cursor = conn.cursor()
    cursor.execute("SELECT MAX(update_date) FROM event_plus WHERE store_type = %s", (brand,))
    latest_date = cursor.fetchone()[0]
    cursor.close()
    conn.close()
    return latest_date.strftime("%Y-%m-%d") if latest_date else "2000-01-01"

# í¬ë¡¤ë§ ì‹¤í–‰
def run_crawling(brand):
    print(f"ğŸ” {brand} í¬ë¡¤ë§ ì‹œì‘...")

    base_url = base_urls[brand]
    latest_update_date = get_latest_update_date(brand)
    print(f"ğŸ“… {brand} ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ: {latest_update_date}")

    page = 1
    previous_page_content = None
    new_data = []

    while True:
        print(f"ğŸ“„ {brand} {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        url = f"{base_url}{page}"

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

        soup = BeautifulSoup(response.text, "html.parser")
        items = soup.find_all("div", class_="col-md-6")

        if not items:
            print(f"ğŸš¨ {brand} ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        current_page_products = "".join(str(item) for item in items)
        if previous_page_content == current_page_products:
            print(f"ğŸš¨ {brand} í˜„ì¬ í˜ì´ì§€ëŠ” ì´ì „ í˜ì´ì§€ì™€ ë™ì¼í•©ë‹ˆë‹¤. í¬ë¡¤ë§ ì™„ë£Œ!")
            break

        for item in items:
            update_date = item.find("small", class_="float-right text-white mr-3")
            title = item.find("strong")
            price_tag = item.find("i", class_="fa-coins")
            price = price_tag.next_sibling.strip() if price_tag else "0ì›"
            event_price_tag = item.find("span", class_="text-muted small")

            # ë¸Œëœë“œë³„ ì´ë²¤íŠ¸ íƒ€ì… í´ë˜ìŠ¤ ì„¤ì •
            if brand == "GS25":
                plus_type_tag = item.find("span", class_="badge bg-gs25 text-white")
            elif brand == "CU":
                plus_type_tag = item.find("span", class_="badge bg-cu text-white")
            else:
                plus_type_tag = item.find("span", class_="badge bg-seven text-white")

            update_date_text = update_date.get_text(strip=True) if update_date else "ë‚ ì§œ ì—†ìŒ"
            formatted_update_date = format_update_date(update_date_text)

            if formatted_update_date <= latest_update_date:
                print(f"âœ… {brand} ì´ë¯¸ ì €ì¥ëœ ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ ì´í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                return

            formatted_price = format_price(price)
            formatted_event_price = format_price(event_price_tag.get_text(strip=True).strip("() ")) if event_price_tag else 0

            new_data.append((
                brand,
                formatted_update_date,
                title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
                formatted_price,
                formatted_event_price,
                plus_type_tag.get_text(strip=True) if plus_type_tag else "ì´ë²¤íŠ¸ ì—†ìŒ"
            ))

        previous_page_content = current_page_products
        page += 1

    if new_data:
        print(f"ğŸ“Œ {brand} ìƒˆë¡œìš´ ë°ì´í„° {len(new_data)}ê°œë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.")
        conn = pymysql.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        insert_sql = """
        INSERT INTO event_plus (store_type, update_date, plus_title, plus_price, plus_event_price, plus_type)
        VALUES (%s, %s, %s, %s, %s, %s)
        """
        cursor.executemany(insert_sql, new_data)
        conn.commit()
        cursor.close()
        conn.close()
        print(f"âœ… {brand} DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    else:
        print(f"âœ… {brand} ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ì—…ë°ì´íŠ¸ í™•ì¸ í›„ í¬ë¡¤ë§ ì‹¤í–‰
def check_and_run_crawling():
    print("ğŸ•› ìì • í¬ë¡¤ë§ í™•ì¸ ì‹œì‘...")
    for brand in base_urls.keys():
        latest_update_date = get_latest_update_date(brand)
        print(f"ğŸ” {brand}ì˜ ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ: {latest_update_date}")

        # ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ í™•ì¸
        url = f"{base_urls[brand]}1"  # ì²« ë²ˆì§¸ í˜ì´ì§€ë§Œ í™•ì¸
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")
        update_date = soup.find("small", class_="float-right text-white mr-3")

        if update_date:
            new_update_date = format_update_date(update_date.get_text(strip=True))
            if new_update_date > latest_update_date:
                print(f"ğŸš€ {brand} ì—…ë°ì´íŠ¸ ê°ì§€ë¨! í¬ë¡¤ë§ ì‹œì‘")
                run_crawling(brand)
            else:
                print(f"âœ… {brand} ì—…ë°ì´íŠ¸ ë³€ê²½ ì—†ìŒ.")
        else:
            print(f"âš ï¸ {brand}ì—ì„œ ì—…ë°ì´íŠ¸ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")

# ìŠ¤ì¼€ì¤„ ì„¤ì • (ë§¤ì¼ ìì • ì‹¤í–‰)
schedule.every().day.at("13:09").do(check_and_run_crawling)

if __name__ == "__main__":
    print("ğŸš€ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì¤‘...")
    
    while True:
        check_and_run_crawling()
        schedule.run_pending()
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ í™•ì¸í•˜ì—¬ ìŠ¤ì¼€ì¤„ ì‹¤í–‰
